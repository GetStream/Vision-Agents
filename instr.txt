 ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
 │ Okay so there's some nuance here, Agent uses the Stream Video transport which has some peer connection     │
 │ handling done internally in Stream Py. The @plugins/openai/stream_agents/plugins/openai/realtime.py        │
 │ also establishes a peer connection to connect and signal with OpenAi which is different. To achieve        │
 │ the effect we're going for, the user joins on the Stream Video service which the agent knows about         │
 │ while the realtime.py file manages the openai peer connection. Think through this and present me with…     │
 └────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

  Ah, now I understand the architecture! Let me think through this properly:

  **🏗️ Architecture Analysis**


  **Two Separate Peer Connections:**

  1. Stream Video Peer Connection (Agent ↔ Stream Video Service)
    • Managed by getstream.Edge() transport
    • Handles user video/audio from Stream Video service
    • Agent receives user tracks through this connection
    • This is where user video tracks come from
  2. OpenAI Peer Connection (Agent ↔ OpenAI Realtime API)
    • Managed by plugins/openai/realtime.py and rtc_manager.py
    • Handles communication with OpenAI's Realtime API
    • This is where we need to send user video TO OpenAI


  **The Missing Link:**

  The issue is that user video from Stream Video needs to be forwarded to the OpenAI peer connection, but these
  are two completely separate WebRTC connections!

  **🎯 The Real Problem**

  Looking at the logs again:
  • ✅ Stream Video connection receives user video: Track received: fcf171a9-f386-45c5-8068-95cf6df05f7b : video
  • ✅ Agent processes the track: 🎥VDP: Processing track: fcf171a9-f386-45c5-8068-95cf6df05f7b
  • ❌ But the Agent's video poller looks for tracks in the Stream Video peer connection
  • ❌ And tries to forward them to OpenAI via start_video_sender()
  • ❌ But start_video_sender() only works within the OpenAI peer connection


  **🔧 The Solution Plan**


  **Phase 1: Fix the Video Detection**

  The Agent's video poller is looking in the wrong peer connection. It should:
  1. Find video tracks from Stream Video connection (where users are)
  2. Forward those tracks to the OpenAI peer connection (where OpenAI is)


  **Phase 2: Implement Cross-Connection Video Forwarding**

  The current start_video_sender() in rtc_manager.py assumes the source track is already in the OpenAI peer
  connection. But we need to:
  1. Read frames from Stream Video track (user's camera)
  2. Create a new track in OpenAI peer connection
  3. Forward frames from Stream Video → OpenAI


  **Phase 3: Architecture Changes Needed**

  Option A: Frame-by-Frame Forwarding (Recommended)

  Stream Video Track → Agent reads frames → Creates OpenAI track → Sends frames to OpenAI

  Option B: Track Bridging

  Stream Video Track → Agent bridges → OpenAI Track (more complex)


  **📋 Detailed Implementation Plan**


  **Step 1: Fix Agent's Video Detection**

  Problem: Agent looks for tracks in Stream Video connection but tries to forward to OpenAI connection
  Solution:
  • Keep the existing _process_track method (it works!)
  • Remove the broken video poller
  • Use the event-based track detection that's already working


  **Step 2: Modify OpenAI RTC Manager**

  Current: start_video_sender() expects a track already in the OpenAI peer connection
  Needed: start_video_sender() should:
  1. Accept a track from Stream Video connection
  2. Read frames from that track
  3. Create/update a track in the OpenAI peer connection
  4. Forward frames between the two connections


  **Step 3: Implement Cross-Connection Frame Forwarding**

  In `rtc_manager.py`:

     1 │async def start_video_sender(self, stream_video_track: MediaStreamTrack, fps: int = 1):
     2 │    """Forward video from Stream Video track to OpenAI peer connection."""
     3 │
     4 │    # 1. Validate we have an OpenAI video sender
     5 │    if not self.send_video or self._video_sender is None:
     6 │        raise RuntimeError("OpenAI video sender not available")
     7 │
     8 │    # 2. Start background task to read from Stream Video track
     9 │    self._video_sender_task = asyncio.create_task(
    10 │        self._forward_stream_to_openai(stream_video_track, fps)
    11 │    )
    12 │
    13 │async def _forward_stream_to_openai(self, stream_track: MediaStreamTrack, fps: int):
    14 │    """Read frames from Stream Video track and forward to OpenAI."""
    15 │
    16 │    interval = max(0.01, 1.0 / max(1, fps))
    17 │
    18 │    while True:
    19 │        try:
    20 │            # Read frame from Stream Video track
    21 │            frame = await stream_track.recv()
    22 │
    23 │            # Convert frame to format OpenAI expects
    24 │            # (This might need format conversion)
    25 │
    26 │            # Send frame to OpenAI peer connection
    27 │            # (This is the tricky part - need to inject into OpenAI's track)
    28 │
    29 │            await asyncio.sleep(interval)
    30 │
    31 │        except Exception as e:
    32 │            logger.error(f"Error forwarding frame: {e}")
    33 │            break


  **Step 4: Handle Frame Format Conversion**

  Challenge: Stream Video frames might be in different format than OpenAI expects
  Solution:
  • Detect frame format from Stream Video
  • Convert to format OpenAI expects (likely RGB24)
  • Handle resolution differences


  **Step 5: Update Agent's Track Processing**

  Current: Agent calls llm.start_video_sender(forward_branch) where forward_branch is from Stream Video
  Needed: This should work, but we need to ensure the RTC manager can handle cross-connection forwarding

  **🚨 Key Challenges to Address**


  **1. Frame Format Compatibility**

  • Stream Video might send H.264/VP8 encoded frames
  • OpenAI might expect raw RGB frames
  • Need format conversion pipeline


  **2. Timing and Synchronization**

  • Stream Video track timing vs OpenAI track timing
  • Frame rate differences
  • Buffer management


  **3. Error Handling**

  • What happens if Stream Video track ends?
  • What happens if OpenAI connection drops?
  • Recovery mechanisms


  **4. Performance**

  • Frame conversion overhead
  • Memory usage for frame buffers
  • CPU usage for real-time conversion


  **🎯 Recommended Approach**

  Phase 1: Fix the immediate issue
  • Remove broken video poller
  • Use existing _process_track method
  • Ensure start_video_sender() gets called

  Phase 2: Implement cross-connection forwarding
  • Modify _forward_video_frames() to read from Stream Video track
  • Add frame format conversion
  • Handle timing and synchronization

  Phase 3: Optimize and test
  • Performance tuning
  • Error handling
  • Edge case testing

  The key insight is that we need to bridge between two separate WebRTC connections, not just forward within a
  single connection. This requires reading frames from one connection and injecting them into another connection's
   track.

 ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
 │ Okay so I think the approach we should go with is frame by frame forwarding from Stream to openai.         │
 │ Please plan the code changes, review it, critique it and improve before writing                            │
 └────────────────────────────────────────────────────────────────────────────────────────────────────────────┘


  **🎯 Frame-by-Frame Forwarding Plan**

  Let me plan the code changes for frame-by-frame forwarding from Stream Video to OpenAI, then critique and
  improve the approach.

  **📋 Initial Plan**


  **1. Modify Agent's Video Processing**

  File: `agents-core/stream_agents/core/agents/agents.py`
  Current Issue: The _process_track method processes tracks but doesn't call start_video_sender
  Change: Ensure video tracks trigger the OpenAI video sender

     1 │async def _process_track(self, track_id: str, track_type: str, participant):
     2 │    """Process video frames from a specific track."""
     3 │    self.logger.info(f"🎥VDP: Processing track: {track_id} from user {getattr(participant, 'user_id',
       │'unknown')} (type: {track_type})")
     4 │
     5 │    # Only process video tracks
     6 │    if track_type != TrackType.TRACK_TYPE_VIDEO:
     7 │        self.logger.debug(f"Ignoring non-video track: {track_type}")
     8 │        return
     9 │
    10 │    track = self._connection.subscriber_pc.add_track_subscriber(track_id)
    11 │    if not track:
    12 │        self.logger.error(f"❌ Failed to subscribe to track: {track_id}")
    13 │        return
    14 │
    15 │    # Verify it's a video track
    16 │    if getattr(track, "kind", None) != "video":
    17 │        self.logger.debug(f"Ignoring non-video track after subscribe: {getattr(track, 'kind', None)}")
    18 │        return
    19 │
    20 │    self.logger.info(f"✅ Subscribed to video track: {track_id}")
    21 │
    22 │    # Create MediaRelay for forwarding
    23 │    relay = getattr(self, "_persistent_media_relay", None)
    24 │    if relay is None:
    25 │        relay = MediaRelay()
    26 │        self._persistent_media_relay = relay
    27 │
    28 │    forward_branch = relay.subscribe(track)
    29 │    self.logger.info(f"🎥 Created forward branch: {type(forward_branch).__name__}")
    30 │
    31 │    # Forward to OpenAI if in STS mode
    32 │    if self.sts_mode and isinstance(self.llm, Realtime):
    33 │        self.logger.info("🎥 Calling llm.start_video_sender...")
    34 │        await self.llm.start_video_sender(forward_branch)
    35 │        self.logger.info("✅ Started OpenAI video sender")
    36 │    else:
    37 │        self.logger.warning(f"🎥 Not in STS mode or LLM is not Realtime: sts_mode={self.sts_mode},
       │llm_type={type(self.llm).__name__}")


  **2. Modify OpenAI RTC Manager**

  File: `plugins/openai/stream_agents/plugins/openai/rtc_manager.py`
  Current Issue: start_video_sender() assumes track is already in OpenAI peer connection
  Change: Accept Stream Video track and forward frames to OpenAI

     1 │async def start_video_sender(self, stream_video_track: MediaStreamTrack, fps: int = 1) -> None:
     2 │    """Forward video frames from Stream Video track to OpenAI via WebRTC.
     3 │
     4 │    This reads frames from the Stream Video track and forwards them through
     5 │    the OpenAI WebRTC connection.
     6 │    """
     7 │    logger.info(f"🎥 start_video_sender called with Stream Video track:
       │{type(stream_video_track).__name__}")
     8 │
     9 │    try:
    10 │        if not self.send_video:
    11 │            raise RuntimeError("Video sending not enabled for this session")
    12 │        if self._video_sender is None:
    13 │            raise RuntimeError("Video sender not available; was video track negotiated?")
    14 │
    15 │        # Validate source track
    16 │        if stream_video_track is None:
    17 │            raise ValueError("Stream Video track cannot be None")
    18 │
    19 │        # Stop any existing video sender task
    20 │        if hasattr(self, '_video_sender_task') and self._video_sender_task:
    21 │            self._video_sender_task.cancel()
    22 │            try:
    23 │                await self._video_sender_task
    24 │            except asyncio.CancelledError:
    25 │                pass
    26 │
    27 │        # Start frame forwarding task
    28 │        self._video_sender_task = asyncio.create_task(
    29 │            self._forward_stream_to_openai(stream_video_track, fps)
    30 │        )
    31 │        self._active_video_source = stream_video_track
    32 │        logger.info(f"✅ Started Stream Video → OpenAI frame forwarding (fps={fps})")
    33 │
    34 │    except Exception as e:
    35 │        logger.error(f"❌ Failed to start video sender: {e}")
    36 │        raise
    37 │
    38 │async def _forward_stream_to_openai(self, stream_track: MediaStreamTrack, fps: int) -> None:
    39 │    """Read frames from Stream Video track and forward to OpenAI."""
    40 │    interval = max(0.01, 1.0 / max(1, fps))
    41 │    frame_count = 0
    42 │
    43 │    try:
    44 │        logger.info(f"🎥 Starting Stream Video → OpenAI forwarding (fps={fps}, interval={interval:.3f}s)")
    45 │
    46 │        while True:
    47 │            try:
    48 │                # Read frame from Stream Video track
    49 │                logger.debug(f"🎥 Reading frame #{frame_count + 1} from Stream Video track...")
    50 │                frame = await asyncio.wait_for(stream_track.recv(), timeout=1.0)
    51 │                frame_count += 1
    52 │
    53 │                logger.info(f"🎥 SUCCESS: Read frame #{frame_count} from Stream Video!")
    54 │                logger.info(f"🎥 Frame details: {frame.width}x{frame.height}, format={frame.format},
       │pts={frame.pts}")
    55 │
    56 │                # Convert frame format if needed
    57 │                converted_frame = await self._convert_frame_for_openai(frame)
    58 │
    59 │                # Forward to OpenAI (this is the tricky part)
    60 │                await self._inject_frame_to_openai(converted_frame)
    61 │
    62 │                logger.debug(f"🎥 Frame #{frame_count} forwarded to OpenAI")
    63 │
    64 │                # Throttle frame rate
    65 │                await asyncio.sleep(interval)
    66 │
    67 │            except asyncio.TimeoutError:
    68 │                logger.warning(f"🎥 Timeout waiting for frame #{frame_count + 1} from Stream Video")
    69 │                continue
    70 │            except Exception as e:
    71 │                logger.error(f"❌ Error reading frame #{frame_count + 1}: {e}")
    72 │                break
    73 │
    74 │    except asyncio.CancelledError:
    75 │        logger.info(f"🎥 Stream Video → OpenAI forwarding cancelled after {frame_count} frames")
    76 │    except Exception as e:
    77 │        logger.error(f"❌ Stream Video → OpenAI forwarding failed after {frame_count} frames: {e}")
    78 │    finally:
    79 │        logger.info(f"🎥 Stream Video → OpenAI forwarding ended. Total frames: {frame_count}")
    80 │
    81 │async def _convert_frame_for_openai(self, frame: VideoFrame) -> VideoFrame:
    82 │    """Convert frame format for OpenAI compatibility."""
    83 │    try:
    84 │        # OpenAI expects RGB24 format
    85 │        if frame.format.name == "rgb24":
    86 │            return frame
    87 │
    88 │        # Convert to RGB24 if needed
    89 │        if hasattr(frame, 'reformat'):
    90 │            converted = frame.reformat(format="rgb24")
    91 │            logger.debug(f"🎥 Converted frame format: {frame.format.name} → rgb24")
    92 │            return converted
    93 │        else:
    94 │            # Fallback: convert via numpy array
    95 │            arr = frame.to_ndarray(format="rgb24")
    96 │            converted = VideoFrame.from_ndarray(arr, format="rgb24")
    97 │            logger.debug(f"🎥 Converted frame via numpy: {frame.format.name} → rgb24")
    98 │            return converted
    99 │
   100 │    except Exception as e:
   101 │        logger.error(f"❌ Frame conversion failed: {e}")
   102 │        # Return original frame as fallback
   103 │        return frame
   104 │
   105 │async def _inject_frame_to_openai(self, frame: VideoFrame) -> None:
   106 │    """Inject frame into OpenAI's video track."""
   107 │    try:
   108 │        # This is the challenging part - we need to inject frames into the existing OpenAI track
   109 │        # The current dummy track generates blue frames, we need to replace that with our frames
   110 │
   111 │        # Option 1: Replace the track's frame generation
   112 │        if hasattr(self._video_track, '_inject_frame'):
   113 │            self._video_track._inject_frame(frame)
   114 │        else:
   115 │            # Option 2: Use a different approach - modify the track's recv method
   116 │            logger.warning("🎥 Frame injection not implemented - OpenAI will see blue frames")
   117 │
   118 │    except Exception as e:
   119 │        logger.error(f"❌ Frame injection failed: {e}")


  **3. Modify the Dummy Video Track**

  File: `plugins/openai/stream_agents/plugins/openai/rtc_manager.py`
  Change: Make the dummy track accept injected frames

     1 │async def _set_video_track(self) -> None:
     2 │    class RealtimeVideoTrack(VideoStreamTrack):
     3 │        kind = "video"
     4 │
     5 │        def __init__(self):
     6 │            super().__init__()
     7 │            self._ts = 0
     8 │            self._injected_frame: Optional[VideoFrame] = None
     9 │            self._frame_lock = asyncio.Lock()
    10 │
    11 │        async def inject_frame(self, frame: VideoFrame) -> None:
    12 │            """Inject a frame from Stream Video."""
    13 │            async with self._frame_lock:
    14 │                self._injected_frame = frame
    15 │
    16 │        async def recv(self):
    17 │            await asyncio.sleep(0.02)
    18 │
    19 │            # Check if we have an injected frame
    20 │            async with self._frame_lock:
    21 │                if self._injected_frame is not None:
    22 │                    frame = self._injected_frame
    23 │                    self._injected_frame = None  # Use once
    24 │                    # Update timing
    25 │                    frame.pts = self._ts
    26 │                    frame.time_base = Fraction(1, 50)
    27 │                    self._ts += 1
    28 │                    return frame
    29 │
    30 │            # Fallback to blue frame
    31 │            width = 640
    32 │            height = 480
    33 │            rgb = np.zeros((height, width, 3), dtype=np.uint8)
    34 │            rgb[:, :, 2] = 255  # Blue in RGB
    35 │            frame = VideoFrame.from_ndarray(rgb, format="rgb24", channel_last=True)
    36 │            frame.pts = self._ts
    37 │            frame.time_base = Fraction(1, 50)
    38 │            self._ts += 1
    39 │            return frame
    40 │
    41 │    self._video_track = RealtimeVideoTrack()
    42 │    self._video_sender = self.pc.addTrack(self._video_track)
    43 │    self._active_video_source: Optional[MediaStreamTrack] = None


  **🔍 Critique and Issues**


  **❌ Problems with Initial Plan:**

  1. Frame Injection Complexity: Injecting frames into an existing track is complex and may not work reliably
  2. Timing Issues: The dummy track's timing may not match Stream Video timing
  3. Format Conversion Overhead: Converting frames in real-time could be expensive
  4. Race Conditions: Multiple frames could arrive faster than they're consumed
  5. Error Handling: What happens if Stream Video track ends or OpenAI connection drops?


  **❌ Missing Considerations:**

  1. Buffer Management: No buffering strategy for frame rate differences
  2. Resolution Handling: No handling of different resolutions between Stream Video and OpenAI
  3. Performance: No optimization for high frame rates
  4. Cleanup: No proper cleanup when forwarding stops


  **🔧 Improved Plan**


  **Better Approach: Replace Track Instead of Inject Frames**

  Instead of trying to inject frames into the dummy track, we should:
  1. Create a new track that reads from Stream Video
  2. Replace the dummy track with the new track using replaceTrack()
  3. Handle the forwarding in the new track's recv() method


  **Improved Implementation:**


     1 │async def start_video_sender(self, stream_video_track: MediaStreamTrack, fps: int = 1) -> None:
     2 │    """Replace OpenAI's dummy track with Stream Video forwarding track."""
     3 │
     4 │    # Create a forwarding track
     5 │    forwarding_track = StreamVideoForwardingTrack(stream_video_track, fps)
     6 │
     7 │    # Replace the dummy track
     8 │    self._video_sender.replaceTrack(forwarding_track)
     9 │    self._active_video_source = stream_video_track
    10 │
    11 │    logger.info(f"✅ Replaced OpenAI track with Stream Video forwarding")
    12 │
    13 │class StreamVideoForwardingTrack(VideoStreamTrack):
    14 │    """Track that forwards frames from Stream Video to OpenAI."""
    15 │
    16 │    kind = "video"
    17 │
    18 │    def __init__(self, source_track: MediaStreamTrack, fps: int = 1):
    19 │        super().__init__()
    20 │        self._source_track = source_track
    21 │        self._fps = max(1, fps)
    22 │        self._interval = 1.0 / self._fps
    23 │        self._ts = 0
    24 │        self._last_frame_time = 0
    25 │
    26 │    async def recv(self):
    27 │        """Read from Stream Video and forward to OpenAI."""
    28 │        now = time.monotonic()
    29 │
    30 │        # Frame rate limiting
    31 │        if now - self._last_frame_time < self._interval:
    32 │            await asyncio.sleep(self._interval - (now - self._last_frame_time))
    33 │
    34 │        try:
    35 │            # Read frame from Stream Video
    36 │            frame = await asyncio.wait_for(self._source_track.recv(), timeout=0.1)
    37 │
    38 │            # Convert format if needed
    39 │            if frame.format.name != "rgb24":
    40 │                frame = frame.reformat(format="rgb24")
    41 │
    42 │            # Update timing for OpenAI
    43 │            frame.pts = self._ts
    44 │            frame.time_base = Fraction(1, self._fps)
    45 │            self._ts += 1
    46 │            self._last_frame_time = time.monotonic()
    47 │
    48 │            return frame
    49 │
    50 │        except asyncio.TimeoutError:
    51 │            # Return black frame if no Stream Video frame available
    52 │            return self._generate_black_frame()
    53 │        except Exception as e:
    54 │            logger.error(f"❌ Error reading from Stream Video: {e}")
    55 │            return self._generate_black_frame()
    56 │
    57 │    def _generate_black_frame(self) -> VideoFrame:
    58 │        """Generate a black frame as fallback."""
    59 │        black_array = np.zeros((480, 640, 3), dtype=np.uint8)
    60 │        frame = VideoFrame.from_ndarray(black_array, format="rgb24")
    61 │        frame.pts = self._ts
    62 │        frame.time_base = Fraction(1, self._fps)
    63 │        self._ts += 1
    64 │        return frame


  **🎯 Final Recommended Approach**

  1. Keep Agent's `_process_track` method - it's working correctly
  2. Modify `start_video_sender()` to create a forwarding track
  3. Use `replaceTrack()` to swap the dummy track with the forwarding track
  4. Handle format conversion in the forwarding track
  5. Add proper error handling and fallback mechanisms
  6. Add comprehensive logging for debugging

  This approach is cleaner, more reliable, and follows WebRTC best practices by using replaceTrack() instead of
  trying to inject frames into an existing track.